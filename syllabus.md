syllabus.md

# Syllabus

## Session 1: What went wrong with this case? 

### Mandatory readings 
* [The case study](/2022-casestudy-algo.pdf)
* Green, B. Z. (2019). [Chapter 6: The Innovative City: The Relationship between Technical and Nontechnical Change in City Government](https://doi.org/10.7551/mitpress/11555.003.0008). In _The Smart Enough City: Putting Technology in Its Place to Reclaim Our Urban Future_. MIT Press. 
* Hao, K. (2020, 20 August). [The UK exam debacle reminds us that algorithms can’t fix broken systems](https://www.technologyreview.com/2020/08/20/1007502/uk-exam-algorithm-cant-fix-broken-system). _MIT Technology Review_. 

### Optional watch 
* Benjamin, R. (2023). [Race to the future?](https://video.publicspaces.net/w/48f7ffdc-73c9-435f-9987-d7142bac3efc). Video. Opening keynote at the Public Spaces Conference in Amsterdam.

## Session 2: Avoiding technical pitfalls in algorithmic systems

### Tools 
* [Hugging Face's work on Model Cards](https://huggingface.co/blog/model-cards)

### Mandatory readings
* Corbett-Davies, S., Pierson, E., Feller, A., Goel, S. (2016, October 17). [A computer program used for bail and sentencing decisions was labeled biased against blacks. It’s actually not that clear](https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/). _The Washington Post_.
* Dressel, J. and Farid, H. (2018). [The accuracy, fairness, and limits of predicting recidivism](https://www.science.org/doi/10.1126/sciadv.aao5580). _Science Advances_.  
* Leufer, D. (2020). [_Myth: AI can be objective or unbiased_](https://www.aimyths.org/ai-can-be-objective-or-unbiased#bias-in-machine-learning). AI Myths.
* Ofqual (2020). [_Executive summary, Student-level equalities analyses for GCSE and A level, Summer 2020_](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/938869/6713_Student-level_equalities_analyses_for_GCSE_and_A_level.pdf). pp. 5-8. 
* O’Neil, C. (2016). Introduction and Chapter 1: “Bomb parts: What is a model?”. In _Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy_.
* Stoyanovich, J. and Arif Khan, F. (2021). [All about that Bias](https://dataresponsibly.github.io/we-are-ai/comics/vol4_en.pdf). _We are AI Comics_, Vol 4.    

### Optional readings
* Angwin, J., Larson, J., Mattu, S., Kirchner, L. (2016, 23 May). [Machine Bias: There’s software used across the country to predict future criminals. And it’s biased against blacks](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing). _ProPublica_. 
* Bennett, S. H. (2020, 20 August). [On A-Levels, Ofqual and Algorithms](https://www.sophieheloisebennett.com/posts/a-levels-2020/). _Sophie Bennett’s blog_. 
* Narayan, A. (2018). [Tutorial: 21 Definitions of Fairness and their politics](https://www.youtube.com/watch?v=wqamrPkF5kk). Video from the 2018 conference on Fairness, Accountability and Transparency of Machine Learning. 
* Ofqual. (2020, 15 April). [_Equality Impact Assessment_](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/879605/Equality_impact_assessment_literature_review_15_April_2020.pdf).
* Selbst, A., boyd, d., Friedler, S., Venkatasubramanian, S., Vertesi, J. (2019). [Fairness and Abstraction in Sociotechnical Systems](https://dl.acm.org/doi/pdf/10.1145/3287560.3287598). Proceedings from the 2019 conference on Fairness, Accountability and Transparency of Machine Learning, Atlanta.  
* Suresh, H. and Guttag, J. (2020). [A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle](https://arxiv.org/abs/1901.10002). Proceedings from the 2020 conference on Fairness, Accountability and Transparency of Machine Learning, Barcelona.  
* Vogl, T. (2021). Algorithmic decision assistance in children’s social care: Questioning the consistency of tools across UK local authorities. [Video](https://youtu.be/zW8KV-‘zjx7g?t=9649) from the conference “L’Etat digital: numérisation de l’administration publique et administration publique numérique”. (2:40:45 - 2:53:45)


## Session 3: Assessing the impacts of an algorithmic system 

### Policy instruments 
* AlgorithmWatch's [Impact Assessment Tool for Public Authorities](https://algorithmwatch.org/en/adms-impact-assessment-public-sector-algorithmwatch/)
* Canada's [Algorithmic Impact Assessment](https://open.canada.ca/aia-eia-js/?lang=en) 
* [Human Rights, Democracy, and the Rule of Law Assurance Framework for AI Systems: A proposal prepared for the Council of Europe's Ad hoc Committee on Artificial Intelligence](https://rm.coe.int/huderaf-coe-final-1-2752-6741-5300-v-1/1680a3f688)
* The Netherlands’ Court of Audit report [_Understanding Algorithms_](https://english.rekenkamer.nl/publications/reports/2021/01/26/understanding-algorithms)
* Supreme Audit Institutions of Finland, Germany, the Netherlands, Norway and the UK, [Auditing machine learning algorithms: A white paper for public auditors](https://www.auditingalgorithms.net/) (2023)
* New Zealand’s [Algorithmic Charter](https://data.govt.nz/assets/data-ethics/algorithm/Algorithm-Charter-2020_Final-English-1.pdf) (for context, see [this presentation page](https://data.govt.nz/toolkit/data-ethics/government-algorithm-transparency-and-accountability/#algorithmCharter)) 
* Mantelero, A., Esposito, M. S. ( July 2021). [An evidence-based methodology for human rights impact assessment (HRIA) in the development of AI data-intensive systems](https://www.sciencedirect.com/science/article/pii/S0267364921000340). Computer Law & Security Review.   

### Mandatory readings
* Ada Lovelace Institute. (2020, 29 April). [_Examining the Black Box: Tools for assessing algorithmic systems_](https://www.adalovelaceinstitute.org/report/examining-the-black-box-tools-for-assessing-algorithmic-systems/), April 29, 2020. 
* Ada Lovelace Institute, AI Now Institute and Open Government Partnership. (2021). [_Executive Summary. Algorithmic Accountability for the Public Sector_.](https://www.opengovpartnership.org/wp-content/uploads/2021/08/executive-summary-algorithmic-accountability.pdf)  
* Constantaras, E., Geiger, G., Braun, J.-C., Mehortra, D., Aung, H. (2023, 6 March). [Inside the Suspicion Machine](https://www.wired.com/story/welfare-state-algorithms/). _Wired_.
* Marda, V., and Narayan, S. (2020). [Data in New Delhi’s Predictive Policing System](https://www.vidushimarda.com/storage/app/media/uploaded-files/fat2020-final586.pdf). Proceedings of the 2020 ACM Conference on Fairness, Accountability, and Transparency. 
* The Global Frontier. (2022, 9 March). [Victims denounce failures in VioGén, the algorithm against gender violence](https://theglobalfrontier.com/victims-denounce-failures-in-viogen-the-algorithm-against-gender-violence/). 

### Optional readings
* Access Now. (October 2022). [Human rights impact assessments for AI: analysis and recommendations](https://www.accessnow.org/wp-content/uploads/2022/11/Access-Now-Version-Human-Rights-Implications-of-Algorithmic-Impact-Assessments_-Priority-Recommendations-to-Guide-Effective-Development-and-Use.pdf). 
* Article 19. (2019). [Governance with teeth: How human rights can strengthen FAT and ethics initiatives on artificial intelligence](https://www.article19.org/wp-content/uploads/2019/04/Governance-with-teeth_A19_April_2019.pdf).
* Braun, J.-C., Constataras, E., Haung, H., Geiger, G., Mehrotra, D., Howden, D. (2023). [Suspicion Machines Methodology: A detailed explainer on what we did and how we did it](https://www.lighthousereports.com/suspicion-machines-methodology/). _LightHouse Reports_. 
* Chowdhury, R., and Williams, J. (2021, July 30). [Introducing Twitter’s first algorithmic bias bounty challenge](https://blog.twitter.com/engineering/en_us/topics/insights/2021/algorithmic-bias-bounty-challenge). 
* Eticas. (2022). [_The External Audit of the VioGén System_](https://eticasfoundation.org/wp-content/uploads/2022/03/ETICAS-FND-The-External-Audit-of-the-VioGen-System.pdf).
* Eubanks, V. (2018). _Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor_. St Martin's Press.
* Gender Shades. [_How well do IBM, Microsoft, and Face++ AI services guess the gender of a face?_](http://gendershades.org/overview.html).
* Metcalf, J., Moss, E., Watkins, E. A., Singh, R., Elish, M. C. (2021). [Algorithmic Impact Assessments and Accountability: The Co-construction of Impacts](https://dl.acm.org/doi/abs/10.1145/3442188.3445935). Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency.  


## Session 4: Working with Different Actors (Affected Communities, Public Procurement)

### Policy instruments
* ECNL's [Framework for Meaningful Engagement: Human Rights Impact Assessments of AI](https://ecnl.org/publications/framework-meaningful-engagement-human-rights-impact-assessments-ai)
* UK’s [Guidelines for AI Procurement](https://www.gov.uk/government/publications/guidelines-for-ai-procurement/guidelines-for-ai-procurement) (2021) 
* Gilman, M. (2023). [Democratizing AI: Principles for Meaningful Public Participation](https://datasociety.net/wp-content/uploads/2023/09/DS_Democratizing-AI-Public-Participation-Brief_9.2023.pdf). Data & Society. 

### Mandatory readings
* Costanza-Chock, S. (2020). [Design Practices: “Nothing about Us without Us.”](https://design-justice.pubpub.org/pub/cfohnud7). In _Design Justice_ (1st ed.).   
* Data Justice Lab. (2021). [_Advancing civic participation in algorithmic decision-making: A guidebook for the public sector_](https://datajusticelab.org/wp-content/uploads/2021/06/PublicSectorToolkit_english.pdf).  
* Elish, M. C. (2020, August 7). [Sepsis Watch in Practice: The labor of disruption and repair in healthcare](https://web.archive.org/web/20210316044439/https://points.datasociety.net/sepsis-watch-in-practice-5b06f88655fe?gi=9e0f4a4c87e9). _Data & Society: Points_. 
* Robinson, D. G. (2022). "Chapter 2: Democracy on the Drawing Board". _Voices in the Code_. Russell Sage Foundation. 
* Sloane, M., Moss, E., Awomolo, O., Forlano, L. (2020). [Participation is not a Design Fix for Machine Learning](https://arxiv.org/abs/2007.02423).


### Optional readings
* Carollo, M., Tanen, B. (2023, 21 March). [How a Group of Health Executives Transformed the Liver Transplant System](https://themarkup.org/organ-failure/2023/03/21/how-a-group-of-health-executives-transformed-the-liver-transplant-system). The Markup. 
* Cardullo, P., Kitchin, Rob. (2019). [Being a 'citizen' in the smart city: up and down the scaffold of smart citizen participation in Dublin, Ireland](https://link.springer.com/article/10.1007/s10708-018-9845-8). _GeoJournal_. 
* Office for Statistics Regulation Authority. (2021, 2 March). [_Ensuring statistical models command public confidence: Learning lessons from the approach to developing models for awarding grades in the UK in 2020, Executive summary_](https://osr.statisticsauthority.gov.uk/publication/ensuring-statistical-models-command-public-confidence/). 
* Singh, R. (2023, 18 August). [Can We Red Team Our Way to AI Accountability?](https://techpolicy.press/can-we-red-team-our-way-to-ai-accountability/). Tech Policy Press. 
* Wylie, B. (2018, 13 August). [Searching for the Smart City's Democratic Future](https://www.cigionline.org/articles/searching-smart-citys-democratic-future/). Centre for International Governance Innovation.
  


## Session 5: Building Accountabiilty: transparency, explainability, appeals processes

### Mandatory readings (policy instruments) 
* [AlgorithmTips.org](http://algorithmtips.org/about/) by the Computational Journalism Lab at Northwestern University (USA)
* AlgorithmWatch Switzerland's [Atlas of Automation](https://algorithmwatch.ch/en/atlas/)
* Amsterdam’s [register](https://algoritmeregister.amsterdam.nl/en/ai-register/) 
* Chile’s [register](https://www.algoritmospublicos.cl/)
* France's [proposed framework for registers](https://guides.etalab.gouv.fr/algorithmes/inventaire/)
* Joshi, D.'s [AI Observatory](https://ai-observatory.in/)
* The Netherland's [algorithm register](https://algoritmes.overheid.nl/nl/algoritme)   
* Ontario’s [register](https://data.ontario.ca/group/artificial-intelligence-andalgorithms) 

### Mandatory readings
* Ananny M, Crawford K. Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability. _New Media & Society_. 2018;20(3):973-989. doi:10.1177/146144481667664
* Green, B., Kak, A. (2021, 15 June). [The False Comfort of Human Oversight as an Antidote to A.I. Harm](https://slate.com/technology/2021/06/human-oversight-artificial-intelligence-laws.html). _Slate_. 
* Jansen, F., Cath, C. (2021). Just Do It: on the limits of governance through AI registers. In _AI Snake Oil, Pseudoscience and Hype, edited by Frederike Kaltheuner_. Meat Space Press. Online link: https://arxiv.org/abs/2109.02944 
* Kolkman, D. (2020, 16 August). [F**ck the algorithm? What the world can learn from the UK A-level grading algorithm fiasco](https://blogs.lse.ac.uk/impactofsocialsciences/2020/08/26/fk-the-algorithm-what-the-world-can-learn-from-the-uks-a-level-grading-fiasco/). LSE Impact Blog. 

### Optional readings
* Feathers, T. (2023, 1 April). [It takes a small miracle to learn basic facts about government algorithms](https://themarkup.org/hello-world/2023/04/01/it-takes-a-small-miracle-to-learn-basic-facts-about-government-algorithms). _The Markup_.
* Wieringa, M. (2023). [“Hey SyRI, tell me about algorithmic accountability”: Lessons from a landmark case](https://www.cambridge.org/core/journals/data-and-policy/article/hey-syri-tell-me-about-algorithmic-accountability-lessons-from-a-landmark-case/22A3086554B0486BB4BBAF2D5A33A3D0). Data & Policy, 5, E2. doi:10.1017/dap.2022.39


## Session 6 : taking down a problematic algorithm and managing the aftermath - Conclusion of the class

### Mandatory readings
* Foxglove. (2020, 17 August). [We put a stop to the A Level grading algorithm!](https://www.foxglove.org.uk/2020/08/17/we-put-a-stop-to-the-a-level-grading-algorithm/). 
* Leufer, D. (2020). [_Myth: AI has agency: headline rephraser tool_](https://www.aimyths.org/ai-has-agency#headline-rephraser). AI Myths. 
* Lulamae, J. (2022). [People are still angry about the UK's 2020 grading algorithm experiment](https://r.algorithmwatch.org/nl3/tnK3o1XF0cFyaf9fHVWtaw?m=AMwAAMY7ZvEAAAAQMhgAAAH66EEAAAAA6uQAAB1gABB0KQBidKt2FY6ovuDyRQ-nbOhWeD8HOwAQJ0I&b=822367f1&e=0b4020f4&x=aALhDVlisnKGlGVxFivX-bk11o6AtUEu-8wM5knkcSk). In Automated Society Newsletter. 
* Ofqual. (2021). [_Decisions on how GCSE, AS and A- level grades will be determined in summer 2021_](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/965005/6747-1_decisions_-_GQ_consultation_on_awarding_grades_in_2021.pdf).
* Poole, S. (2020, 3 September). [Steven Poole’s word of the day: 'Mutant algorithm': boring B-movie or another excuse from Boris Johnson?](https://www.theguardian.com/books/2020/sep/03/mutant-algorithm-boring-b-movie-or-another-excuse-from-boris-johnson). _The Guardian_.  
* Redden, J. (2022, 21 September). [Government's use of automated decision-making systems reflects systemic issues of injustice and inequality](https://theconversation.com/governments-use-of-automated-decision-making-systems-reflects-systemic-issues-of-injustice-and-inequality-185953). _The Conversation_.

### Optional reading
* Edwards, L. (2022). [_The EU AI Act: a summary of its significance and scope. Ada Lovelace Institute_](https://www.adalovelaceinstitute.org/wp-content/uploads/2022/04/Expert-explainer-The-EU-AI-Act-11-April-2022.pdf) 
* Green, B. Z. (2019). [Chapter 2: The Livable City: The Limits and Dangers of New Technology](https://doi.org/10.7551/mitpress/11555.003.0004). In _The Smart Enough City: Putting Technology in Its Place to Reclaim Our Urban Future_. MIT Press. 
* Vervloesem, K. (2020, 6 April). [How Dutch activists got an invasive fraud detection algorithm banned](https://algorithmwatch.org/en/syri-netherlands-algorithm/). AlgorithmWatch blog.  

 
## Readings about technical aspects of algorithms/AI

### Introductory resources
* Jist Studios, BBC Ideas. (2019, 19 September). “What exactly is an algorithm?” [[Video](https://www.bbc.co.uk/ideas/videos/what-exactly-is-an-algorithm/p07nw8ny)].  
* Leufer, D. (2020). [Myth: the term AI has a clear meaning](https://www.aimyths.org/the-term-ai-has-a-clear-meaning). AI Myths.  
* Stoyanovich, J. and Arif Khan, F. (2021). [“What is AI?”](https://dataresponsibly.github.io/we-are-ai/comics/vol1_en.pdf). _We are AI Comics_, Vol 1.  

### MOOCs to go deeper 
* [Elements of AI](https://course.elementsofai.com/).
* [Open Classrooms course "Initiez-vous à l'intelligence artificielle", in French](https://openclassrooms.com/fr/courses/6417031-objectif-ia-initiez-vous-a-lintelligence-artificielle)
