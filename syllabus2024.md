syllabus.md

# Syllabus

## Session 1: What went wrong with this case? - Introduction

### Mandatory readings 
* [The case study](/2022-casestudy-algo.pdf)
* Green, B. Z. (2019). [Chapter 6: The Innovative City: The Relationship between Technical and Nontechnical Change in City Government](https://doi.org/10.7551/mitpress/11555.003.0008). In _The Smart Enough City: Putting Technology in Its Place to Reclaim Our Urban Future_. MIT Press. 
* Hao, K. (2020, 20 August). [The UK exam debacle reminds us that algorithms can’t fix broken systems](https://www.technologyreview.com/2020/08/20/1007502/uk-exam-algorithm-cant-fix-broken-system). _MIT Technology Review_. 
* Toh, A. (2014, 5 January). [The Algorithms Too Few People Are Talking About](https://www.hrw.org/news/2024/01/05/algorithms-too-few-people-are-talking-about). _Human Rights Watch_. 

### Optional watch 
* Benjamin, R. (2023). [Race to the future?](https://video.publicspaces.net/w/48f7ffdc-73c9-435f-9987-d7142bac3efc). Video. Opening keynote at the Public Spaces Conference in Amsterdam.

## Session 2: Avoiding tech pitfalls - errors, choices, biases, (justice?)

### Tools 
* [Hugging Face's work on Model Cards](https://huggingface.co/blog/model-cards)

### Mandatory readings
* Corbett-Davies, S., Pierson, E., Feller, A., Goel, S. (2016, October 17). [A computer program used for bail and sentencing decisions was labeled biased against blacks. It’s actually not that clear](https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/). _The Washington Post_.
* Dressel, J. and Farid, H. (2018). [The accuracy, fairness, and limits of predicting recidivism](https://www.science.org/doi/10.1126/sciadv.aao5580). _Science Advances_.  
* Leufer, D. (2020). [_Myth: AI can be objective or unbiased_](https://www.aimyths.org/ai-can-be-objective-or-unbiased#bias-in-machine-learning). AI Myths.
* Ofqual (2020). [_Executive summary, Student-level equalities analyses for GCSE and A level, Summer 2020_](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/938869/6713_Student-level_equalities_analyses_for_GCSE_and_A_level.pdf). pp. 5-8. 
* O’Neil, C. (2016). Introduction and Chapter 1: “Bomb parts: What is a model?”. In _Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy_.
* Ziosi, M. and Pruss, D. (2024). [Evidence of What, for Whom? The Socially Contested Role of Algorithmic Bias in a Predictive Policing Tool](https://doi.org/10.1145/3630106.3658991). In _The 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT '24), June 3-6, 2024, Rio de Janeiro, Brazil._ ACM, New York, NY, USA. 

### Optional readings
* Angwin, J., Larson, J., Mattu, S., Kirchner, L. (2016, 23 May). [Machine Bias: There’s software used across the country to predict future criminals. And it’s biased against blacks](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing). _ProPublica_.
* Balayn, A., Gürses, S. (2021). [Beyond Debiasing: Regulating AI and its inequalities](https://edri.org/wp-content/uploads/2021/09/EDRi_Beyond-Debiasing-Report_Online.pdf). _EDRi_.
* Bennett, S. H. (2020, 20 August). [On A-Levels, Ofqual and Algorithms](https://www.sophieheloisebennett.com/posts/a-levels-2020/). _Sophie Bennett’s blog_. 
* D'Ignazio, C. and Klein, L. (2020). [2. Collect, Analyze, Imagine, Teach](https://data-feminism.mitpress.mit.edu/pub/ei7cogfn/release/4). In _Data Feminism_. MIT Press. 
* Narayan, A. (2018). [Tutorial: 21 Definitions of Fairness and their politics](https://www.youtube.com/watch?v=wqamrPkF5kk). Video from the 2018 conference on Fairness, Accountability and Transparency of Machine Learning. 
* Ofqual. (2020, 15 April). [_Equality Impact Assessment_](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/879605/Equality_impact_assessment_literature_review_15_April_2020.pdf).
* Selbst, A., boyd, d., Friedler, S., Venkatasubramanian, S., Vertesi, J. (2019). [Fairness and Abstraction in Sociotechnical Systems](https://dl.acm.org/doi/pdf/10.1145/3287560.3287598). Proceedings from the 2019 conference on Fairness, Accountability and Transparency of Machine Learning, Atlanta.  
* Stoyanovich, J. and Arif Khan, F. (2021). [All about that Bias](https://dataresponsibly.github.io/we-are-ai/comics/vol4_en.pdf). _We are AI Comics_, Vol 4.
* Suresh, H. and Guttag, J. (2020). [A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle](https://arxiv.org/abs/1901.10002). Proceedings from the 2020 conference on Fairness, Accountability and Transparency of Machine Learning, Barcelona.  
* Wang, A., Kapoor, S., Barocas, S., Narayanan, A. (2023). [Against Predictive Optimization:
On the Legitimacy of Decision-Making Algorithms that Optimize Predictive Accuracy](https://predictive-optimization.cs.princeton.edu/). 

## Session 3: Assessing the impacts of an algorithmic system 

### Policy tools 
* Canada's [Algorithmic Impact Assessment](https://open.canada.ca/aia-eia-js/?lang=en) - the world's first institutionalized algorithmic impact assessment in the public sector. See for instance the impact assessment for the ["advanced analytics triage of overseas temporary resident visa applications"](https://open.canada.ca/data/en/dataset/6cba99b1-ea2c-4f8a-b954-3843ecd3a7f0).
* Dutch Data Protection Authority - Department for the Coordination of Algorithmic Oversight (DCA). (2024). [_AI & Algorithmic Risks Report Netherlands_](https://www.autoriteitpersoonsgegevens.nl/uploads/2024-01/AI%20%26%20Algorithmic%20Risks%20Report%20Netherlands%20-%20winter%202023%202024.pdf) - for an example of how national authorities can practice "algorithmic oversight".
* Eticas, BID. (2021). [Robot Laura Auditoría Algorítmica](https://publications.iadb.org/publications/spanish/viewer/Robot-laura-auditoria-algoritmica.pdf). Another applied example of an audit. 
* Supreme Audit Institutions of Finland, Germany, the Netherlands, Norway and the UK, [Auditing machine learning algorithms: A white paper for public auditors](https://www.auditingalgorithms.net/) (2023) - for an example of how Courts of Audits can tackle the issue. 
* US Department of State. (July 25, 2024). [Risk Management Profile for Artificial Intelligence and Human Rights](https://www.state.gov/risk-management-profile-for-ai-and-human-rights/) - with a focus on human rights.
* New Zealand’s [Algorithmic Charter](https://data.govt.nz/assets/data-ethics/algorithm/Algorithm-Charter-2020_Final-English-1.pdf) (for context, see [this presentation page](https://data.govt.nz/toolkit/data-ethics/government-algorithm-transparency-and-accountability/#algorithmCharter)). 
* Mantelero, A., Esposito, M. S. ( July 2021). [An evidence-based methodology for human rights impact assessment (HRIA) in the development of AI data-intensive systems](https://www.sciencedirect.com/science/article/pii/S0267364921000340). Computer Law & Security Review.   

### Mandatory readings
* Ada Lovelace Institute. (2020, 29 April). [Examining the Black Box: Tools for assessing algorithmic systems](https://www.adalovelaceinstitute.org/report/examining-the-black-box-tools-for-assessing-algorithmic-systems/).
* Ada Lovelace Institute, AI Now Institute and Open Government Partnership. (2021). [Executive Summary. Algorithmic Accountability for the Public Sector.](https://www.opengovpartnership.org/wp-content/uploads/2021/08/executive-summary-algorithmic-accountability.pdf).
* Costanza-Chock, S., Raji, I. D., Buolamwini, J. (2022). [Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem](https://dl.acm.org/doi/abs/10.1145/3531146.3533213). FAccT '22: Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency.
* Groves, L., Metcalf, J., Kennedy, A., Vecchione, B., & Strait, A. (2024). [Auditing work: Exploring the New York City algorithmic bias audit regime](https://doi.org/10.1145/3630106.3658959). In *Proceedings of the Association for Computing Machinery*. Association for Computing Machinery. 
* Constantaras, E., Geiger, G., Braun, J.-C., Mehortra, D., Aung, H. (2023, 6 March). [Inside the Suspicion Machine](https://www.wired.com/story/welfare-state-algorithms/). _Wired_.
* Tapasya, Sambhav., K., Joshi, D. (2024). (2024, January 24). ["How an algorithm denied food to thousands of poor in India’s Telangana"](https://www.aljazeera.com/economy/2024/1/24/how-an-algorithm-denied-food-to-thousands-of-poor-in-indias-telangana). _AlJazeera_. 


### Optional readings
* Braun, J.-C., Constataras, E., Haung, H., Geiger, G., Mehrotra, D., Howden, D. (2023). [Suspicion Machines Methodology: A detailed explainer on what we did and how we did it](https://www.lighthousereports.com/suspicion-machines-methodology/). _LightHouse Reports_. 
* Chowdhury, R., and Williams, J. (2021, July 30). [Introducing Twitter’s first algorithmic bias bounty challenge](https://blog.twitter.com/engineering/en_us/topics/insights/2021/algorithmic-bias-bounty-challenge). 
* Eubanks, V. (2018). _Automating Inequality: How High-Tech Tools Profil, Police, and Punish the Poor_. St Martin's Press. 
* Gender Shades. [_How well do IBM, Microsoft, and Face++ AI services guess the gender of a face?_](http://gendershades.org/overview.html).
* Marda, V., and Narayan, S. (2020). [Data in New Delhi’s Predictive Policing System](https://www.vidushimarda.com/storage/app/media/uploaded-files/fat2020-final586.pdf). Proceedings of the 2020 ACM Conference on Fairness, Accountability, and Transparency. 
* Valdivia, A., Hyde-Vaamonde, C., García Marcos, J. (2024). [Judging the algorithm: Algorithmic accountability on the risk assessment tool for intimate partner violence in the Basque Country](https://link.springer.com/content/pdf/10.1007/s00146-024-02016-9.pdf). _AI & Society_.
* Varon, J. and Peña, P. (2022). [Not My A.I.
Towards Critical Feminist Frameworks To Resist Oppressive A.I. Systems](https://www.hks.harvard.edu/sites/default/files/2023-11/22_10JoanaVaron.pdf). Carr Center for Human Rights Policy, Harvard Kennedy School, Harvard University.

### Optional general reads on policy
* Access Now. (2024). [_Radiografía normativa: ¿dónde, qué y cómo se está regulando la inteligencia artificial en América Latina?_](https://www.accessnow.org/wp-content/uploads/2024/02/LAC-Reporte-regional-de-politicas-de-regulacion-a-la-IA.pdf). _For an overview of policy developments in Latin America. 
* The resources of the [GovAI Coalition](https://www.sanjoseca.gov/your-government/departments-offices/information-technology/ai-reviews-algorithm-register/govai-coalition#deliverables) (US-based).

## Session 4: Building Accountabiilty: transparency, appeals, public procurement

### Policy tools
#### Registers
* Amsterdam’s [register](https://algoritmeregister.amsterdam.nl/en/ai-register/)
* Chile’s [register](https://www.algoritmospublicos.cl/)
* France's [proposed framework for registers](https://guides.etalab.gouv.fr/algorithmes/inventaire/)
* The UK's [Algorithmic Transparency Recording Standard Hub](https://www.gov.uk/government/collections/algorithmic-transparency-recording-standard-hub)
* Joshi, D.'s [AI Observatory](https://ai-observatory.in/)
* Public Law Project, [The Tracking Automated Government register](https://publiclawproject.org.uk/resources/the-tracking-automated-government-register/) (UK). 
* The Netherland's [algorithm register](https://algoritmes.overheid.nl/nl/algoritme)

#### Procurement
* ChileCompra's [Standard Bidding Terms for algorithms and artificial intelligence with ethical requirements](https://goblab.uai.cl/en/chilecompra-presents-unprecedented-standard-bidding-terms-for-algorithms-and-artificial-intelligence-with-ethical-requirements/)
* [EU model contractual AI clauses to pilot in procurements of AI](https://public-buyers-community.ec.europa.eu/communities/procurement-ai/resources/eu-model-contractual-ai-clauses-pilot-procurements-ai).
* UK’s [Guidelines for AI Procurement](https://www.gov.uk/government/publications/guidelines-for-ai-procurement/guidelines-for-ai-procurement) (2021) 

### Mandatory readings
* Ananny M, Crawford K. [Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability](https://journals.sagepub.com/doi/10.1177/1461444816676645). _New Media & Society_. 2018;20(3):973-989.
* Green, B., Kak, A. (2021, 15 June). [The False Comfort of Human Oversight as an Antidote to A.I. Harm](https://slate.com/technology/2021/06/human-oversight-artificial-intelligence-laws.html). _Slate_. 
* Jansen, F., Cath, C. (2021). [Just Do It: on the limits of governance through AI registers](https://arxiv.org/abs/2109.02944). In _AI Snake Oil, Pseudoscience and Hype, edited by Frederike Kaltheuner_. Meat Space Press.  
* Kolkman, D. (2020, 16 August). [F**ck the algorithm? What the world can learn from the UK A-level grading algorithm fiasco](https://blogs.lse.ac.uk/impactofsocialsciences/2020/08/26/fk-the-algorithm-what-the-world-can-learn-from-the-uks-a-level-grading-fiasco/). LSE Impact Blog. 
* Riley, S. (2024). [Overriding (in)justice: Pretrial risk assessment administration on the frontlines](https://doi.org/10.1145/3630106.3658920). In _Proceedings of the Association for Computing Machinery_. Association for Computing Machinery.

### Optional readings
* Elish, M. C. (2020, August 7). [Sepsis Watch in Practice: The labor of disruption and repair in healthcare](https://web.archive.org/web/20210316044439/https://points.datasociety.net/sepsis-watch-in-practice-5b06f88655fe?gi=9e0f4a4c87e9). _Data & Society: Points_. 
* Feathers, T. (2023, 1 April). [It takes a small miracle to learn basic facts about government algorithms](https://themarkup.org/hello-world/2023/04/01/it-takes-a-small-miracle-to-learn-basic-facts-about-government-algorithms). _The Markup_.
* Wright, L., Muenster, R. M., Vecchione, B., Qu, T., Cai, P. (S.), Smith, A., Comm 2450 Student Investigators, Metcalf, J., & Matias, J. N. (2024). [Null compliance: NYC Local Law 144 and the challenges of algorithm accountability](https://dl.acm.org/doi/10.1145/3630106.3658920). In _Proceedings of the Association for Computing Machinery_. Association for Computing Machinery. 

## Session 5: Designing participatory algorithmic governance

### Practical Guidance & Frameworks
* ECNL's [Framework for Meaningful Engagement: Human Rights Impact Assessments of AI](https://ecnl.org/publications/framework-meaningful-engagement-human-rights-impact-assessments-ai).
* Data Justice Lab. (2021). [_Advancing civic participation in algorithmic decision-making: A guidebook for the public sector_](https://datajusticelab.org/wp-content/uploads/2021/06/PublicSectorToolkit_english.pdf).
* Gilman, M. (2023). [Democratizing AI: Principles for Meaningful Public Participation](https://datasociety.net/wp-content/uploads/2023/09/DS_Democratizing-AI-Public-Participation-Brief_9.2023.pdf). Data & Society.

### Mandatory readings
* Blair Attard-Frost's work on AI Countergovernance, either as [a written piece](https://www.midnightsunmag.ca/ai-countergovernance/) or as [a podcast](https://www.techpolicy.press/imagining-ai-countergovernance/).
* Costanza-Chock, S. (2020). [Design Practices: “Nothing about Us without Us.”](https://design-justice.pubpub.org/pub/cfohnud7). In _Design Justice_ (1st ed.).
* Hu, W.and Singh, R. (2024). [Enrolling Citizens: A Primer on Archetypes of Democratic Engagement with AI](https://datasociety.net/wp-content/uploads/2024/06/DS_Enrolling-Citizens-Primer_FINAL.pdf). _Data & Society_.  
* Ofqual. (2020). [Analysis of Consulation Responses: Exceptional arrangements for exam grading and assessment in 2020](https://www.gov.uk/government/consultations/exceptional-arrangements-for-exam-grading-and-assessment-in-2020#history). 
* Robinson, D. G. (2022). "Chapter 2: Democracy on the Drawing Board". _Voices in the Code_. Russell Sage Foundation. 
* Sloane, M., Moss, E., Awomolo, O., Forlano, L. (2020). [Participation is not a Design Fix for Machine Learning](https://arxiv.org/abs/2007.02423).

### Optional readings
* Carollo, M., Tanen, B. (2023, 21 March). [How a Group of Health Executives Transformed the Liver Transplant System](https://themarkup.org/organ-failure/2023/03/21/how-a-group-of-health-executives-transformed-the-liver-transplant-system). The Markup. 
* Cardullo, P., Kitchin, Rob. (2019). [Being a 'citizen' in the smart city: up and down the scaffold of smart citizen participation in Dublin, Ireland](https://link.springer.com/article/10.1007/s10708-018-9845-8). _GeoJournal_. 
* Office for Statistics Regulation Authority. (2021, 2 March). [_Ensuring statistical models command public confidence: Learning lessons from the approach to developing models for awarding grades in the UK in 2020, Executive summary_](https://osr.statisticsauthority.gov.uk/publication/ensuring-statistical-models-command-public-confidence/). 
* Singh, R. (2023, 18 August). [Can We Red Team Our Way to AI Accountability?](https://techpolicy.press/can-we-red-team-our-way-to-ai-accountability/). Tech Policy Press. 
* Wylie, B. (2018, 13 August). [Searching for the Smart City's Democratic Future](https://www.cigionline.org/articles/searching-smart-citys-democratic-future/). Centre for International Governance Innovation.

### Optional - Examples of campaigns
* Information about the EPIC's [complaint's to the FTC about "Fraud Detect"](https://statescoop.com/automated-public-benefit-fraud-detection-state-ftc-complaint/)
* Surveillance Resistance Lab. [MyCity, INC. A Case Against "CompStat Urbanism"](https://surveillanceresistancelab.org/wp-content/uploads/MyCityINC_March2024.pdf).

## Session 6 : Taking down a system and managing the aftermath - Conclusion

### Examples of campaigning & redress
* Albarado, S. (2023, August 7). [Disabled Arkansans obtain settlement and program improvements in lawsuit against DHS officials](https://arktimes.com/arkansas-blog/2023/08/07/disabled-arkansans-obtain-settlement-and-program-improvements-in-lawsuit-against-dhs-officials). In _Arkansas Time_. 
* Oosting, J. (2024, January 30). [Michiganders falsely accused of jobless fraud to share in €20M settlement](https://www.bridgemi.com/michigan-government/michiganders-falsely-accused-jobless-fraud-share-20m-settlement). In _Bridge Michigan_.
* Vervloesem, K. (2020, April 6). [How Dutch activists got an invasive fraud detection algorithm banned](https://algorithmwatch.org/en/syri-netherlands-algorithm/). AlgorithmWatch blog.  

### Mandatory readings
* Ehsan, U., Singh, R., Metcalf, J., & Riedl, M. (2022). [The algorithmic imprint](https://doi.org/10.1145/3531146.3533186). In _Proceedings of the Association for Computing Machinery_. Association for Computing Machinery. 
* Foxglove. (2020, 17 August). [We put a stop to the A Level grading algorithm!](https://www.foxglove.org.uk/2020/08/17/we-put-a-stop-to-the-a-level-grading-algorithm/). 
* Leufer, D. (2020). [_Myth: AI has agency: headline rephraser tool_](https://www.aimyths.org/ai-has-agency#headline-rephraser). AI Myths. 
* Ofqual. (2021). [_Decisions on how GCSE, AS and A- level grades will be determined in summer 2021_](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/965005/6747-1_decisions_-_GQ_consultation_on_awarding_grades_in_2021.pdf).
* Poole, S. (2020, September 3). [Steven Poole’s word of the day: 'Mutant algorithm': boring B-movie or another excuse from Boris Johnson?](https://www.theguardian.com/books/2020/sep/03/mutant-algorithm-boring-b-movie-or-another-excuse-from-boris-johnson). _The Guardian_.  
* Redden, J. (2022, September 21). [Government's use of automated decision-making systems reflects systemic issues of injustice and inequality](https://theconversation.com/governments-use-of-automated-decision-making-systems-reflects-systemic-issues-of-injustice-and-inequality-185953). _The Conversation_.

### Optional readings
* Green, B. Z. (2019). [Chapter 2: The Livable City: The Limits and Dangers of New Technology](https://doi.org/10.7551/mitpress/11555.003.0004). In _The Smart Enough City: Putting Technology in Its Place to Reclaim Our Urban Future_. MIT Press. 
* Lulamae, J. (2022). [People are still angry about the UK's 2020 grading algorithm experiment](https://r.algorithmwatch.org/nl3/tnK3o1XF0cFyaf9fHVWtaw?m=AMwAAMY7ZvEAAAAQMhgAAAH66EEAAAAA6uQAAB1gABB0KQBidKt2FY6ovuDyRQ-nbOhWeD8HOwAQJ0I&b=822367f1&e=0b4020f4&x=aALhDVlisnKGlGVxFivX-bk11o6AtUEu-8wM5knkcSk). In Automated Society Newsletter. 
